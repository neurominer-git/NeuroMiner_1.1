Display training results {#3.7_display_training_results}
------------------------

Once initialization, preprocessing, and model training has been
performed, the results can then be displayed using this option. The
results display interface can be understood in three main divisions: 1)
the accuracy and model performance (initial screen); 2) the model
performance across the selected hyperparameters; 3) results concerning
the individual features in the analysis.

#### Accuracy and Model Performance

See Fig. [1](#fig:Display_ClassPlot){reference-type="ref"
reference="fig:Display_ClassPlot"}. The performance of a model can be
fundamentally described by the degree to which the predictions match the
target scores (e.g., how many people are classified correctly or how
closely the features and target are related in a regression). In
NeuroMiner, this is represented in multiple ways that each address a
different aspect of the relationship between subjects, predictions, and
targets.

#### Model Comparisons

See Fig.[8](#fig:Display_Statistical_Comp){reference-type="ref"
reference="fig:Display_Statistical_Comp"}. When there is more than one
analysis, statistical comparisons can be conducted between the model
performances using a variety of statistics using the \"Statistical
Comparisons\" option. The results of these comparisons are output in
either excel spreadsheet files (i.e., xlsx) or in text files (i.e., csv)
for Linux and Mac. The menu is quite intuitive (i.e., as below) where
you select the analyses you want to compare on the right, you press the
\"Save As\" button to choose a name, and then you select the \"Compare\"
button to generate the files. The files will be generated in your
working directory.

See Fig. [9](#fig:Display_Visual_Comp){reference-type="ref"
reference="fig:Display_Visual_Comp"}. The user can also perform \"Visual
Comparisons\". The user needs to select some performance measures that
they want to compare visually in the top-half of the pop-up box using
the \"Select\" column tick-boxes, then selects the analyses in the
lower-half of the pop-up box. Then simply press \"Visualise\". If the
bar plots are not good then the user can change it to \"Line plots\" and
then press \"Visualise\" again.

#### Classification Performance

See Fig. [2](#fig:Display_CVperf){reference-type="ref"
reference="fig:Display_CVperf"}. NeuroMiner was built to work with
hyperparameter optimisation within a nested-cross validation framework
as a standard. In this plot, you will see the inner-cycle, CV1, mean
test performance plotted in blue and then the outer cycle, CV2, mean
test performance plotted in red. Each point represents a hyperparameter
combination. This gives an indication of how the hyperparameter
combinations are influencing the test performance. On the right, the
display also shows heatmaps that show how the performance in the CV1/CV2
folds changes over the permutations.

#### Generalization Error

See Fig. [3](#fig:Display_GenError){reference-type="ref"
reference="fig:Display_GenError"}. Another way to look at CV2 test
performance is with the generalization error, which is calculated in
NeuroMiner as the CV1-test performance minus the CV2 test performance.

#### Model Complexity

See Fig. [4](#fig:Display_Complexity){reference-type="ref"
reference="fig:Display_Complexity"}. In NeuroMiner, complexity is
defined as the percentage of support vectors divided by the number of
subjects in the tested fold. This feature is useful to determine how
varying parameter combinations may affect the fit of the model.

#### Ensemble Diversity

See Fig. [5](#fig:Display_EnsembleDiversity){reference-type="ref"
reference="fig:Display_EnsembleDiversity"}. This is a measure of Shannon
entropy across CV1 models for each CV2 partition. A higher entropy means
higher disagreement between the CV1 models. This measure is useful
because increased entropy will often result in a better fitting model at
the CV2 level. It is useful to note that entropy can be maximised as a
performance criterion in addition to accuracy.

#### Model Selection Frequency

See Fig. [6](#fig:Display_ModelSelection){reference-type="ref"
reference="fig:Display_ModelSelection"}. Within an ensemble framework,
the models that are selected within the inner, CV1, cycle are then
applied to the outer test data. For example, if it is a 10x10 CV1
framework with a winner-takes-all method of model selection, then 100
models will be applied to the held-out CV2 test fold. This plot displays
the percentage of times that each parameter combination was selected.

#### Multi-Group Results

If a multi-group analysis is conducted then the results display will
change to accommodate the groups. See figure
[7](#fig:Display_MultiGroup){reference-type="ref"
reference="fig:Display_MultiGroup"}.

![Option 1: classification
plot](Images/Display_ClassPlot.pdf){#fig:Display_ClassPlot
width="1\\linewidth"}

![Option 2: Cross-validation
Performance.](Images/Display_CVperf.png){#fig:Display_CVperf
width="1\\linewidth"}

![Option 3: Generalization
Error.](Images/Display_GenError.png){#fig:Display_GenError
width="1\\linewidth"}

![Option 4: Model
Complexity](Images/Display_Complexity.png){#fig:Display_Complexity
width="1\\linewidth"}

![Option 4: Ensemble
Diversity](Images/Display_EnsembleDiversity.png){#fig:Display_EnsembleDiversity
width="1\\linewidth"}

![Option 5: Model Selection
Frequency](Images/Display_ModelSelection.png){#fig:Display_ModelSelection
width="1\\linewidth"}

![The multi-group
display](Images/Display_MultiGroup.png){#fig:Display_MultiGroup
width="1\\linewidth"}

![Within the main results display, the user can go to the
\"Comparisons\" menu item and then select the option to calculate
statistical differences between separate
models](Images/Display_Statistical_Comp.png){#fig:Display_Statistical_Comp
width="1\\linewidth"}

![Within the main results display, the user can go to the
\"Comparisons\" menu item and then select the option to display visual
comparisons between separate
models](Images/Display_Visual_Comp.png){#fig:Display_Visual_Comp
width="1\\linewidth"}
