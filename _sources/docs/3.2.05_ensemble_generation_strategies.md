### Ensemble generation strategies {#3.2.05_ensemble_generation_strategies}

NeuroMiner automatically works with ensemble learning at the CV2 level
across the CV1 folds of a nested, repeated cross-validation design (see
Fig. [\[fig:nested-CV\]](#fig:nested-CV){reference-type="ref"
reference="fig:nested-CV"}), but it also includes the ability to apply
filters and wrappers that optimise predictions by selecting features
within these folds (for more information, see
[link](https://en.wikipedia.org/wiki/Ensemble_learning) and [Kolhavi &
John](robotics.stanford.edu/users/gjohn/ftp/papers/wrap-final.ps)).

#### Filter-based ensemble generation

NeuroMiner has two methods of applying filters: 1) as part of the
preprocessing of features by pre-selecting variables based on their
relationship to the target (see
[\[3.2.02.10_preprocessing_pipeline\]](#3.2.02.10_preprocessing_pipeline){reference-type="ref"
reference="3.2.02.10_preprocessing_pipeline"}; 2) as an ensemble method
where different variable sets (variable subspaces) can be chosen based
on the performance of the machine learning training algorithm. This
latter case is known as constrained feature optimization because the
variable sets are produced based on the performance of an algorithm
(e.g., correlation), the sets are evaluated with the algorithm, and then
the models corresponding to well-performing variable sets are chosen as
an ensemble.

The first step is to turn the option to yes specifying the \"train
filter methods on CV1 partitions\". This will generate a menu with the
following:

1: Train filter methods on CV1 partitions\
2: Use algorithm output scores or label predictions\
3: Specify filter type\
4: Define minimum number of features to be selected\
5: Specify subspace optimization strategy\
6: Define target population for computing optimization\
7: Define subspace stepping

**1: Train filter methods on CV1 partitions** Turn constrained feature
optimization filtering on or off.

**2: Use algorithm output scores or label predictions** Choose the
feature sets based on majority voting of the predicted labels (hard
decision ensemble) or based on an average of the predicted probabilities
from the machine learning algorithm (soft decision ensemble).

**3: Specify filter type** Specify what filter will be used to obtain
the feature sets. These methods are the same as in the preprocessing
filter outlined in section
[\[3.2.02.10_preprocessing_pipeline\]](#3.2.02.10_preprocessing_pipeline){reference-type="ref"
reference="3.2.02.10_preprocessing_pipeline"}.

**4: Define minimum number of features to be selected** This gives the
option to define a minimum number of features that are retained when
choosing feature sets.

**5: Specify subspace optimization strategy** Once the subspaces are
defined and the models are evaluated, this section will define which
subspaces/models are retained. There is the option to retain one model
of the best performing subspace, or an ensemble of models across
well-performing subspaces.

1: Subspace with maximum argmax. Argmax (arguments of the maxima) are
the points of the domain of some function at which the function values
are maximised. This option picks the maximum argmax across the feature
subspaces (winner-takes-all).\
2: Subspace ensemble with maximum argmax Picks the X most predictive
subspaces with reference to the argmax.\
3: Subspace ensemble with maximum argmax above a percentile Picks the
top X% of subspaces.\
4: All-subspace ensemble Uses all subspaces when training the model.

**6: Define target population for computing optimization** You can
choose the best performing models using the CV1 training data, the CV1
test data, or the CV1 training & the test data. This means that the
subspaces will be selected on the basis of how they predict the labels
in each of these data folds. Selection based on training and test data
is recommended.

**7: Define subspace stepping** This option defines how the subspaces
are formed. The features are ranked based on the association between
them and the target variable, and then they are divided into subspaces
based on blocks of X% of features; e.g., blocks of 10% of features would
divide the data into the top 10% performing features, then the top 20%
of features, then the top 30% of features, and so on.

It is important to note that once the feature subspaces are defined, the
features within the winning subspaces can then be used in a wrapper to
further optimise performance.

#### Wrapper-based model selection

The wrapper methods in NeuroMiner use either Greedy feature selection or
simulated annealing to select feature combinations that maximise the
predictive accuracy of a model in the CV1 data. You will see the
following menu:

1 : Deactivate wrapper methods\
2 : Wrapper type \[Greedy feature selection (Use CV1 test data; Stop at
k=50% of features; Cross-CV1 PFE off)\]\
3 : CV1 data partitions for optimization \[CV1 test data\]\
4 : Cross-CV1 Feature selection \[Cross-CV1 PFE off\]

**2: Wrapper type** Select Greedy feature selection or simulated
annealing. Each option will lead to a new menu as follows:

*Greedy Feature Selection*\
1 : Search direction \[ Forward \]\
2 : Early stopping \[ Stop at 50% of feature pool \]\
3 : Feature stepping \[ Each feature will be evaluated \]

The search direction must be provided (forward or backward), which
determines whether an empty subspace is filled with features that are
the most predictive of the target variable (forward selection) or
whether all the variables are entered into the subspace and then
variables are removed until the optimal model is found (backward)

Early stopping is when variables are added up to a total percentage of
the feature pool and then no more variables are added. This feature is
useful when you want to find parsimonious solutions and also because
restricting the feature pool can lead to better solutions.

Feature stepping can also be changed from adding/removing single
features and then testing the model, or adding/removing percentages of
features before testing the model. Adding percentages (e.g., 5%)
normally results in a faster processing time.

*Simulated Annealing*\
1 : Sparsity constant \[ 0.01 \]\
2 : Starting temperature \[ 1 \]\
3 : Stopping temperature \[ 0.005 \]\
4 : Alpha \[ 0.9 \]\
5 : Maximum \# of iterations \[ 2000 \]\
6 : Maximum \# of repetitions in Temperature T \[ 100 \]\
7 : Minimum \# of repetitions until solution is accepted in T \[ 30 \]\
8 : k constant (smaller k ==$>$ less solutions accepted) \[ 10 \]

Simulated annealing is a probabilistic technique for approximating the
global maximum of a function
[wiki](https://en.wikipedia.org/wiki/Simulated_annealing). The settings
are standard parameters for any simulated annealing analysis.

**3: CV1 data partitions for optimization** Choose either the CV1
training, test, or test & training data to optimize the feature set.

**4: Cross-CV1 feature selection** In a nested cross-validation design,
you will have a selection of features for each of the CV1 partitions.
This step allows you to select features across the CV1 folds and will
reveal another menu:

1 : Optimize feature selection across CV1 partitions \[ yes \]\
2 : Probabilistic feature extraction mode \[ % Cross-CV1 feature
selection agreement \]\
3 : Apply consistency-based ranking to \[ Prune unselected features from
each model and retrain models \]

*Probabilistic feature extraction*\
1 : % Cross-CV1 feature selection agreement\
2 : Absolute number of most consistently selected features\
3 : Percentage of most consistently selected features

The first option allows you to select features that occur across CV1
partitions at a certain percentage rate. For example, select only
features that appear across CV1 folds 75% of the time or above this.
Sometimes the threshold that is set does not return any features, and
therefore you will also be asked to define a tolerance value for this
circumstance. For example, if the 75% criterion is not met, then reduce
this by 25% (i.e., so then you are effectively selecting features 50% of
the time). You will also be asked to define a minimum number of features
that must be selected each time (e.g., 1 feature).

Option 2, \"Absolute number of most consistently selected features\",
just orders the features based on the amount of times they were selected
across the CV1 folds. Then you can select however many features that
occur at the top of the list (e.g., you could select the top 10 most
consistently selected features).

Option 3, \"Percentage of most consistently selected features\". This
option sorts in the same way as described for absolute number of
consistently selected features and then establishes a cut-off. The
features above this cut-off are kept. For example, if 90% is the cut-off
then it will select the top 10% most consistently selected features
across the CV1 folds.

*Apply consistency-based ranking*

1 : Retrain all CV1 models after pruning them from unselected features\
2 : Retrain all CV1 models using the same selected feature space

Once the features are selected, you can either \"1: Retrain all CV1
models\" after pruning the unselected features. Or you can completely
retrain the models using the single optimized feature set that was found
in the previous steps.
