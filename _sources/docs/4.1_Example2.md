(4.0_Example)=
# Example

<!-- For a first-time user, NeuroMiner might be best described using an
example. This section will run through a simple example using structural
neuroimaging data.

### Example: Structural NeuroImaging Data

The sample data for this example is open source and can be found in
([link](http://cobre.mrn.org/)). We processed the data with the Voxel
Based Morphometry (VBM) toolbox and we're going to use the mwrp1
(grey-matter segmented, realigned, warped, modulated) images. Each voxel
of these images represents the relative volume of grey matter at any
location. The aim of the analysis is to predict who is an individual
with a diagnosis of schizophrenia and who doesn't have a diagnosis
purely from the grey matter volumes. Based on previous research, the
hypothesis is that those with schizophrenia will be able to be
distinguished with approximately 70% accuracy.

If the SPM path has been added to NeuroMiner, then the main menu will
look like the following:

1 : Load data for model discovery and cross-validation\
2 : Load NeuroMiner structure\
3 : Change working directory\
4 : Utilities

The first step is to enter the data using the first option, which will
display the following menu:

1 : Define machine learning framework \[ classification \]\
2 : Select data input format \[ nifti \]\
3 : Define no. of samples and sample identifiers \[ ??? \]\
4 : Involve unlabeled cases in the analyses \[ no \]\
5 : Select space-defining image \[ ? \]\
6 : Describe data \[ ? \]

So, we're going to leave the machine learning framework as
classification because this is a classification problem, and since we
haven't conducted an SPM analysis we will also opt to import NIFTI files
directly. I'll then select the third option to define the number of
samples and will define the following:

Number of groups? : 2\
Define name of group number 1 : SCZ\
Define name of group number 2 : CTRL

I've entered the individuals which a schizophrenia diagnosis first
because then the sensitivity accuracy of the analysis will be based on
this group rather than the controls. Once this data is entered, then the
menu changes to the following:

1 : Define machine learning framework \[ classification \]\
2 : Select data input format \[ nifti \]\
3 : Modify no. samples \[ N=2 \] and sample identifiers \[ SCZ, CTRL \]\
4 : Involve unlabeled cases in the analyses \[ no \]\
5 : Select space-defining image \[ ? \]\
6 : Map image files to samples \[ ? \]\
7 : Describe data \[ ? \]

I don't want to \"Involve unlabeled cases in the analysis\", so I'll
then select the space-defining image (i.e., the mask; keep in-mind that
the images will be resliced to this). I'll either be able to select a
region from the WFU PickAtlas or just use my own mask. For this
analysis, I don't want to select individual regions, so I'm going to
select option 2 and use a pre-prepared ICBM mask in my templates folder.
Then because the template is not completely binary and there is a
transition from 0 to 1 at the border of the brain, I'm going to select a
threshold of 0.5 and then define the regions that are greater than or
equal to this threshold (i.e., $>$=).

I then select option 6 to \"Map image files to samples\" and a file
selector box will appear that is the same as in SPM. At the top of the
box, you will see what the box is for: \"Select nifti for sample 1:
SCZ\". To select the images, I could navigate to the directory on the
left and select images, but we recommend to have a file that stores the
absolute paths of each of the images in a separate text file. This is
because it keeps a record of what has been entered, but mainly because
the order of the images is very important for later steps. As an
example, my file looks like this:

/NeuroDiagStor1/COBRE/Data/0040000/mwrp1mprage.nii\
/NeuroDiagStor1/COBRE/Data/0040001/mwrp1mprage.nii\
/NeuroDiagStor1/COBRE/Data/0040002/mwrp1mprage.nii\
\...

Once I have this file, I then press the small button on the bottom left
called \"Ed\" and an editor will appear. Then I paste the files into
this editor, first for the SCZ group and then for the CTRL group. I then
go back to the main menu and enter the global grey matter volumes (or
the intracranial volume) with data that I have loaded into the MATLAB
workspace. It is critical that these global volume data are in the same
order as the subjects that I have just entered into NeuroMiner. If they
are in the wrong order then the results of the analysis will be invalid.

I then go back to the main menu and there will now be an option to
\"Inspect images\". I press this, check that the images have been read
correctly and then enter a few images to visually inspect. In this case,
1:4. This will display the space-defining mask image as well as the
first 4 of my mwrp images. They look good, so I will then choose
\"Describe data\" and enter: COBREmwrp1. After this, I will \"IMPORT\"
the images and this will lead to reslicing and masking with the
space-defining image (i.e., my ICBM template).

After the data has been imported, a \"MODALITIES\" menu will appear with
the analysis descriptor (i.e., COBREmwrp1) at the top, then some details
about the images. It is important to check that these settings are
correct, and the dimensionality of the images. NeuroMiner automatically
reports the number of features and the percentage of zero/NaN (\"not a
number\") features. This is important for later analysis (e.g., if there
are NaN features then they probably need to be excluded. It is also
important to note that for this analysis, no covariates have been found
in the NM workspace because they have not been entered yet.

I want to enter covariates of age and sex, so I will choose option \"5:
Add covariate(s) to NM workspace\". These covariates must be already
loaded prior to entering into NeuroMiner, otherwise you will have to
exit the program, load the covariates, and then enter the program again.
After pressing 5, I will then enter the variable that I've previously
loaded \"cobre_covars\" and then name the covariates \"one-by-one\"
because there are only two: age and sex. It's important to note that
these variables will just be entered to the NM structure and the **will
not be controlled for these variables** until you add this as a
preprocessing step.

So now that my variables are in then I'll choose \"6: Finish data import
for discovery & cross-validation analysis\". This means that the data
will be saved in the NeuroMiner structure and I don't be able to modify
it or add to it again. NeuroMiner locks the data down like this in order
to make sure that latter analyses and organisation, relate to the same
dataset. So, now that the data are in, I'll be taken to the main
NeuroMiner menu where I can set-up my analyses.

1 : Inspect data used for model discovery and cross-validation\
2 : Set up NM parameter workspace\
3 : Initialize & manage analyses\
4 : Preprocess features\
5 : Train supervised classifiers\
6 : Load NeuroMiner structure\
7 : Save NeuroMiner structure\
8 : Clear NeuroMiner structure\
9 : Change working directory\
10 : Utilities

As described in section [\[mainmenu\]](#mainmenu){reference-type="ref"
reference="mainmenu"}, I will need to first set up a parameter template
with the settings that I want, initialize the analysis, and then run it.
For this reason, I'll go to \"2: Set up NM parameter workspace\" and it
will take me to the following menu (see section
[\[3.2_define_parameter_template\]](#3.2_define_parameter_template){reference-type="ref"
reference="3.2_define_parameter_template"}):

1 : Cross-validation settings \[ ??? \]\
2 : Preprocessing pipeline \[ \... \]\
3 : Classification algorithm \[ \... \]\
4 : Learning algorithm parameters \[ \... \]\
5 : Ensemble generation strategies \[ \... \]\
6 : Visualization options \[ \... \]\
7 : Model saving options \[ ??? \]\
8 : Save parameter template\
9 : Load training template\
10 : Define verbosity level \[ Detailed output \]

The fields with question marks \"???\" need to be completed before the
analysis can be initialized or run, whereas the other sections are
pre-set with default settings. The first thing that I need to do is to
set-up cross-validation in step 1. Pressing this enters the
cross-validation menu (see section
[\[3.2.01_cross-validation_settings\]](#3.2.01_cross-validation_settings){reference-type="ref"
reference="3.2.01_cross-validation_settings"}):

1 : Select cross-validation framework \[ (Pooled) cross-validation \]\
2 : Define no. of CV2 permutations \[ P2 = 10 \]\
3 : Define no. of CV2 folds \[ K2 = 10 \]\
4 : Define no. of CV1 permutations \[ P1 = 10 \]\
5 : Define no. of CV1 folds \[ K1 = 10 \]\
6 : Equalize class sizes at the CV1 cycle by undersampling \[no\]\
7 : Build CV2/CV1 structure\
8 : Load CV structure\
9 : Save CV structure

It's automatically set-up with repeated, nested, cross-validation, which
is the gold standard cross-validation set-up currently (see Fig.
[\[fig:nested-CV\]](#fig:nested-CV){reference-type="ref"
reference="fig:nested-CV"}). I could change these settings, but in this
case I think they're great for my sample so I'll just choose the option
to \"7: Build CV2/CV1 structure\". This will then build the structure
and then I'll go back to the paramter settings menu by simply pressing
the enter key because here the default option is to quit the menu.

I'll then enter the pre-processing menu by typing \"2\" and hitting
enter. This will show me a few default settings:

=========================\
CV-PREPROCESSING PIPELINE\
=========================\
$>>$ Step 1: Scale \[ from 0 to 1 \], zero-out completely non-finite
features\
Step 2: Prune non-informative columns from matrix \[ Zero Var, Nan, Inf
\]

And the other settings that I could add are:\
1 : Enable spatial operations using Spatial OP Wizard\
2 : Add preprocessing step\
3 : Remove preprocessing step\
4 : Insert preprocessing step\
5 : Replace current preprocessing step\
6 : Modify current preprocessing step\
7 : Next step $>>$\
8 : Change order of preprocessing steps

For this analysis, I don't want to scale the data first because in
previous tests I found that this introduced some noise in the ventrices
and I don't need to prune non-informative columns because I know that
there are no NaN or Zero variance features in the data from when I did
the import. So I'll remove both of these by pressing \"3\" and then
\"3\" again.

Now that the CV-PREPROCESSING PIPELINE is empty, I'll add the operations
I want using option \"2: Add preprocessing step\" and will see this
menu:

1 : Correct data for nuisance covariates using partial correlations\
2 : Apply dimensionality reduction method to data\
3 : Standardize data\
4 : Scale data\
5 : Normalize to group mean\
6 : Normalize data to unit vector\
7 : Apply binning method to data\
8 : Prune non-informative columns from data matrix\
9 : Rank / Weight features\
10 : Correct group offsets from global group mean\
11 : Extract variance components from data

Then I'll add the option to \"Correct data for nuisance covariates\"
(section
[\[3.2.02.2_preprocessing_pipeline\]](#3.2.02.2_preprocessing_pipeline){reference-type="ref"
reference="3.2.02.2_preprocessing_pipeline"}). I'll then select option 1
and select both covariates by entering \"1:2\", and I'll keep the rest
of the default settings so that age and sex will the regressed from the
entire sample. After I've set these options, I'll then quit to go back
to the preprocessing menu.

After covarying for nuisance covariates, I now want to perform a
dimensionality reduction so I will select \"2: add preprocessing step\"
and then \"Apply dimensionality reduction method to data\" (section
[\[3.2.02.3_preprocessing_pipeline\]](#3.2.02.3_preprocessing_pipeline){reference-type="ref"
reference="3.2.02.3_preprocessing_pipeline"}). There are a lot of
options, but basically it seems that PCA works well most of the time so
I'll select \"1: Principal components analysis\". Then I'll keep all the
defaults because these work well too, and return to the main menu. In
this step, if I had have wanted to optimise over a number of different
component reductions, then I could have defined the energy as 1,
returned to the preprocessing menu, added another step, and then added
extracted component subspaces and defined something like \[0.2 0.4 0.6
0.8\] to see how the algorithm performs when I retain from 20 to 80% of
the energy.

Then I want to add some scaling because I'm going to use an SVM and the
data must be scaled for this and also because it makes the results more
interpretable. So I'll select \"4: Scale data\" and again just accept
the defaults. Now the CV PREPROCESSING PIPELINE should include:

[=========================\
CV-PREPROCESSING PIPELINE\
=========================]{style="color: blue"}\
$>>$ Step 1: Correct for nuisance covariates \[age,sex\]\
Step 2: Apply dimensionality reduction \[ PCA \]\
Step 3: Scale \[ from 0 to 1 \], zero-out completely non-finite features

And this will be applied within each of the CV1 folds.

I'll then go back to the main parameter template menu by pressing enter
and will see this:

1 : Cross-validation settings \[ \... \]\
2 : Preprocessing pipeline \[ \... \]\
3 : Classification algorithm \[ \... \]\
4 : Learning algorithm parameters \[ \... \]\
5 : Ensemble generation strategies \[ \... \]\
6 : Visualization options \[ \... \]\
7 : Model saving options \[ ??? \]\
8 : Save parameter template\
9 : Load training template\
10 : Define verbosity level \[ Detailed output \]

Then I want to define the classification algorithm by pressing \"3:
Classification algorithm\" (see section
[\[3.2.03_classification_algorithm\]](#3.2.03_classification_algorithm){reference-type="ref"
reference="3.2.03_classification_algorithm"}). The default in NeuroMiner
is to implement a SVM, and the only thing that I want to change is the
performance measure to Balanced Accuracy instead of accuracy. I can do
this by selecting the \"performance criterion\" and then selecting
\"Balanced accuracy: BAC\...\". Then I'll return to the parameter
template menu by pressing enter again.

Here I could change the learning algorithm parameters (see section
[\[3.2.04_learning_algorithm_parameters\]](#3.2.04_learning_algorithm_parameters){reference-type="ref"
reference="3.2.04_learning_algorithm_parameters"}), but I want to keep
the defaults again. It's important to note here that I'm keeping a range
of C parameters that will be optimised during the nested
cross-validation. I could also enable some filters and wrappers (see
section
[\[3.2.05_ensemble_generation_strategies\]](#3.2.05_ensemble_generation_strategies){reference-type="ref"
reference="3.2.05_ensemble_generation_strategies"}), but these are more
advanced features that can be addressed in a further tutorial. I could
also enable visualization options (such as the derivation of Z or
p-values; see section
[\[3.2.06_visualization_options\]](#3.2.06_visualization_options){reference-type="ref"
reference="3.2.06_visualization_options"}), but because I have enabled
PCA these will be overly conservative and so I won't use them.

The final thing to do is to specify \"7: Model saving options\". For
this, I'll choose that I don't want to save the models because it will
take up a lot of space (i.e., more than 10 000 models). Then I'll define
a name anyway because this is required in NeuroMiner. Then I'll return
to the MAIN INTERFACE menu of NeuroMiner by pressing the enter key until
I get back here:

1 : Inspect data used for model discovery and cross-validation\
2 : Set up NM parameter workspace\
3 : Initialize & manage analyses\
4 : Preprocess features\
5 : Train supervised classifiers\
6 : Load NeuroMiner structure\
7 : Save NeuroMiner structure\
8 : Clear NeuroMiner structure\
9 : Change working directory\
10 : Utilities

From this menu, I'll then need to \"3: initialize & manage analyses\"
(see section
[\[3.3_initialize_delete_analyses\]](#3.3_initialize_delete_analyses){reference-type="ref"
reference="3.3_initialize_delete_analyses"}) where I'll see this:

Define analysis identifier \[ ? \]\
Provide analysis description \[ ? \]\
Specify parent directory of the analysis \[ ? \]\
PROCEED $>>>$

I'll then define the identifier as \"COBRE_SVM_analysis1\", provide a
description of the analysis \"This is a test analysis using the COBRE
dataset\", and specify the parent directory of the analysis using the
file selector box that pops up. Then I'll \"PROCEED\" and a directory
will be created in the directory. Within this directory you will find a
log file that outlines the operating system and the settings that I have
just defined. After assuring myself that these are ok, then I'll go back
to the main menu.

Now I have the option to \"4: preprocess features\" (see section
[\[3.4_preprocess_features\]](#3.4_preprocess_features){reference-type="ref"
reference="3.4_preprocess_features"}) first, which will apply all the
preprocessing steps I defined to the CV1 folds. Or alternatively, I
could simply \"5: Train supervised classifiers\" (see section
[\[3.5_train_supervised_classifiers\]](#3.5_train_supervised_classifiers){reference-type="ref"
reference="3.5_train_supervised_classifiers"} from scratch and
automatically preprocess the features as part of this. I want to
preprocess first here because then I can run other parameters later with
this preprocessed data, so I'll select \"4: Preprocess features\". Which
will activate:

1 : Overwrite existing preprocessed data files \[ no \]\
2 : Use existing preprocessing parameter files, if available \[ no \]\
3 : Write out computed parameters to disk (may require A LOT of disk
space) \[ no \]\
4 : Select CV2 partitions to operate on \[ 100 selected \]\
5 : PROCEED $>>>$

I'll keep all the defaults here, but I'll select \"4: Select CV2
partitions to operate on\", then hit \"4: Deselect all CV2 positions\",
and then I'll choose \"3: Select single perm / fold range\" and enter
\[1,1,1,10\] to select the first permutation and folds 1-10. This is
because selecting all permutations will take a lot of time and is not
required for this example. Then I'll hit enter to go back to the main
menu and then \"5: PROCEED\". The analysis should run now, and I'll go
and get some lunch.

When I get back, the analysis should have finished and NeuroMiner should
have returned to the main menu. All the preprocessed data files should
be stored in the subdirectory for this analysis that I activated
previously. Now all that's left to do is to actually train the
classifiers. This can be done by selecting \"5: Train supervised
classifiers\" then \"2: Operation mode of ML training module\" and
select \"2: Compute using existing preprocdata-MATs\" and when the file
selector box appears, I'll select the preprocessed datamats from the
previous analysis. Then I'll select \"PROCEED\", check to get an initial
impression of the accuracy , and then I'll go and write that fellowship
application.

When I come back again, the analysis is finished and the training
datamats are stored in the directory as well. I can now see the results
by going to \"7: Display the training results\" (see
[\[3.7_display_training_results\]](#3.7_display_training_results){reference-type="ref"
reference="3.7_display_training_results"}), including the accuracies
(see
[\[fig:Display_ClassPlot\]](#fig:Display_ClassPlot){reference-type="ref"
reference="fig:Display_ClassPlot"}). This is interesting and so now I
want to visualise the weights on the brain. For this, I'll go back to
the main menu and select \"Visualise Classifiers\" (see section
[\[3.6_visualize_classifiers\]](#3.6_visualize_classifiers){reference-type="ref"
reference="3.6_visualize_classifiers"}) and select the data that I've
processed and hit proceed. This will then create some images that I can
view with a viewer such as MRICRON, FSL, SPM, MANGO, or others.

If this is your own data, hopefully then you have found out about some
problem and you can now go and publish. -->
