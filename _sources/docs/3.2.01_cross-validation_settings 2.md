### Cross-validation settings {#3.2.01_cross-validation_settings}

Cross-validation is a fundamental core of machine learning analysis
because it facilitates out-of-sample generalization and parameter tuning
[link](https://en.wikipedia.org/wiki/Cross-validation_(statistics)).
NeuroMiner has been built to flexibly create custom cross-validation
schemes depending on your data problem. NeuroMiner has been built around
performing repeated, nested, cross-validation, which is a very robust
method that increases the likelihood of generalization.

The cross-validation works by first defining settings of the following
options:\
1 : Select cross-validation framework \[ (Pooled) cross-validation \]\
2 : Define no. of CV2 permutations \[ P2 = 1 \]\
3 : Define no. of CV2 folds \[ K2 = 10 \]\
4 : Define no. of CV1 permutations \[ P1 = 1 \]\
5 : Define no. of CV1 folds \[ K1 = 10 \]\
6 : Equalize class sizes at the CV1 cycle by undersampling \[no\]\
7 : Build CV2/CV1 structure\
8 : Load CV structure\
9 : Save CV structure

The user then must build the cross-validation structure in step 7 before
exiting the menu. The cross-validation framework will then be built
within the NM structure.

#### Select cross-validation framework {#3.2.01.1_cross-validation_settings}

This option allows the user to select either k-fold cross-validation
scheme (including leave-one-out) or a leave-group-out analysis with the
following options:

1 \| Pooled\
2 \| Outer Leave-Group-Out/Inner pooled\
3 \| Nested Leave-Group-Out\
4 \| Outer Leave-Group-Out/Inner Leave-Group-In\
NeuroMiner has been built around the gold standard of repeated, nested
cross-validation [(Filzmoser et al.,
2009)](https://en.wikipedia.org/wiki/Cross-validation_(statistics)) and
therefore the default options are reflective of this. For a description
of cross-validation, you could check-out the supplementary material of
this publication: [Koutsouleris et al.,
2016](http://thelancet.com/journals/lanpsy/article/PIIS2215-0366(16)30171-7/fulltext).
In brief, leave-one-out cross-validation and k-fold cross-validation
risk overfitting (i.e., your models not generalizing past your sample)
in the context of (hyper)parameter optimisation (e.g., optimising C
parameters) or the optimisation of feature selection based on the
predictive accuracy on the held-out dataset (e.g., using wrappers
discussed in the \"ensemble\" section of this manual). Nested
cross-validation mitigates against overfitting and provides models that
are more likely to generalise--which is the central aim of machine
learning and science generally.

![Nested cross-validation in NeuroMiner. **A:** The data is split into k
number of folds in the outer CV2 set and a CV2 test set is held-out. The
rest of the data goes into the 'nest' where it is again subdivided into
k-folds, a test fold is held-out, and the rest of the data is used for
training. Models associated with each parameter combination (e.g., C or
epsilon) are created in the CV1 training data and applied to the CV1
test data. **B:** accuracy is assessed across all parameter combinations
for all CV1 models. The optimal parameter combination is chosen based on
a criteria set by the user. For each CV1 fold, the training and test
data, which has been preprocessed, is retrained with the winning
parameter or parameter combination. The models are then applied to the
held-out CV2 test data. In cases there are many (hyper)parameter
combinations or when there is variable selection procedure then the user
can specify to keep a percentage of top performing models for each fold
(this is discussed in the section of the manual on the learning
algorithm parameters).](Images/nested-CVpng){#fig:nested-CV
width="1\\linewidth"}

A depiction of nested cross-validation is represented in Figure
[1](#fig:nested-CV){reference-type="ref" reference="fig:nested-CV"}. In
NeuroMiner the outer cross-validation cycle of a nested cross-validation
scheme is designated as **\"CV2\"** and the inner cross-validation folds
are designated as **\"CV1\"**.Models are trained in the CV1 cycle and
then the best performing models are applied to the CV2 data. This
separation of CV2 and CV1 data avoids overfitting.

Option \"1 \| Pooled\" means that the outer and inner cross-validation
folds will be automatically and randomly defined. Option \"2 \| Outer
Leave-Group-Out/Inner pooled\" means that the outer CV2 folds will be
defined by the user using a vector defining groups (e.g., if there are
different sites then this will test the ability of the classifiers that
are trained with pooled site data to generalise to new sites). Option
\"3 \| Nested Leave-Group-Out\" means that both the outer and inner
cycles will be separated based on the grouping vector that the user
enters (e.g., in the site example, this will mean that the models will
be optimised to generalise across sites in the CV1 cycle and then
applied to different sites in the CV2 cycle). Option \"4 \| Outer
Leave-Group-Out/Inner Leave-Group-In\" means that the models in the
inner CV1 cycle will only be trained in one group prior to being applied
to all other groups (e.g., this will test the robustness of models from
single sites). See [1](#fig:nested-CV){reference-type="ref"
reference="fig:nested-CV"} for more details and imagine that the folds
are sites or diagnoses or any other grouping variable.

To further avoid overfitting and increase the generalizability of the
model predictions, NeuroMiner also allows the user to implement
'repeated' cross-validation for each of the CV cycles. This involves
shuffling the subjects prior to the definition of the folds so that
there are different subjects in the test/training folds. This is usually
done for both the CV2 and CV1 cycles.

A clear depiction of how repeated cross-validation works is difficult,
but it is important to keep in-mind that there will be a lot of models
produced under these schemes; e.g., if you have a standard 10x10 CV1/CV2
framework and you're choosing the optimal model for each fold, then
you'll have 10,000 models. A way to understand how these are applied to
an individual subject is to consider that all models in which that
subject is **not** included in the CV1 training/testing are applied to
them--because if they were included then this would be information
leakage.

On the surface, having 10,000 models is not so great for the
interpretation or understanding of a phenomena under investigation.
However, doing this can be really good for generalization when it's
unlikely that there is a singular model that can be found and we need
something that practically works--e.g., for many complex problems in
psychiatry. For an interesting discussion on this topic, see [Breiman et
al.,
2001](http://www.stat.uchicago.edu/~lekheng/courses/191f09/breiman.pdf).

#### Define number of CV2 permutations {#3.2.01.2_cross-validation_settings}

Define the number of repeats that you want to happen to the outer, CV2
data. As stated above, the subjects are shuffled and new folds are
created. The number of permutations is based on a balance between the
computational time that your analysis is taking and the stability of the
predictions from your modeling.

#### Define number of CV2 folds {#3.2.01.3_cross-validation_settings}

Define the number of folds that you want the outer data to be split
into. If -1 is entered then a leave-one-out cross-validation is
implemented and the permutations option is not possible (because it
doesn't matter if you shuffle the data if every single subject is left
out). The number of folds is usually based on the number of subjects
that you have.

#### Define number of CV1 permutations {#3.2.01.4_cross-validation_settings}

Define the number of inner, CV1 permutations in the inner folds. This is
based on the computational time and also the model stability.

#### Define number of CV1 folds {#3.2.01.5_cross-validation_settings}

Define the number of inner, CV1 folds. The number of folds is usually
based on the number of subjects you have and what kind of analyses that
you are conducting. For an analytic example of where this is important
see the section on ensemble generation strategies--e.g., using wrappers
that are optimising features based on the subjects in the CV1 test
folds.

#### Equalize class sizes at the CV1 cycle by undersampling {#3.2.01.6_cross-validation_settings}

Situations where there are unbalanced groups (e.g., less people
transitioning to illness than people who do not) pose a problem for
support vector machines and other analysis techniques. Usually, a model
is learned that preferences the majority group and then you'll get
unbalanced predictions (e.g., your specificity will be high, but
sensitivity low). In NeuroMiner there are three ways to deal with this:
1) undersampling the majority group during training; 2) weighting the
SVM hyperplane; 3) Oversampling using Adaptive synthetic sampling
approach. This option allows the user to undersample the majority group
so that the groups are balanced when the models are created in the CV1
folds.

After selecting the option, the user will be prompted with this:
\"Enable class size balancing at the CV1 level\", and enabling this
option will lead to a prompt: \"Perform histogram euqalization using the
target label or some other covariate\", you can choose depending on
which parameter you want to undersampling your algoritm. Target label
will ask: \"Target ratio bigger / smaller class (1 - classes have sample
size after balancing) (Default: 1.5 )\" This is the ratio of the bigger
to smaller class sample sizes. Equal class sizes can be selected by
entering in \"1\". The default is 1.5 because otherwise the models may
not generalise well. If the dataset has any covariates, selecting
\"Covariate\" option will require you to select the covariates (integers
or range).

The next option gives the user the option to add the excluded CV1
subjects in the training folds to the CV1 test folds. This option is
potentially important for potential generalizability of the models to
the CV2 folds (and ultimately to real-life).

#### Build CV2/CV1 structure {#3.2.01.7_cross-validation_settings}

Once the options have been selected above, then the user needs to build
the CV2/CV1 structure. This literally creates all the folds and indexes
the subjects that are to be included in the folds. These can be found in
the NM structure (e.g., NM.cv).

#### Load CV structure {#3.2.01.8_cross-validation_settings}

If you have saved the CV structure for your data with the subjects
indexed into each fold/permutation, you can load it using this function.
This can be useful if you establish a new analysis with new settings,
but you want to load the CV structure that you made before.

#### Save CV structure {#3.2.01.9_cross-validation_settings}

This gives the user the option to save the indexed CV structure
containing the folds and permutations for later use. It is saved in a
.mat file.
