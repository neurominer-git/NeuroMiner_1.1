(3.7_mainmenu_display_training_results)=
# Result Viewer

Once initialization, preprocessing, and model training have been performed, the results can then be viewed using this option. The results display interface can be understood in three main divisions:
1. the accuracy and model performance (initial screen)
2. the model performance across the selected hyperparameters
3. results concerning the individual features in the analysis

## Accuracy and model performance
See {numref}(fig:Display_ClassPlot): The performance of a model can be fundamentally described by the degree to which the predictions match the target scores (e.g., how many people are classified correctly or how closely the features and target are related in a regression). In NeuroMiner, this is represented in multiple ways that each address a different aspect of the relationship between subjects, predictions, and targets.

```{figure} Images/Display_ClassPlot.png
---
alt: Neurominer result viewer, classification plot
name: fig:Display_ClassPlot
---
Result viewer option 1: Classification plot
```

## Model comparisons
The user can perform statistical or visual comparisons when there is more than one analysis:

```{figure} Images/NM_resultviewer_comparison_dropdown.png
---
alt: Neurominer result viewer, comparisons options
name: fig:NM_resultviewer_comparsion_dropdown
---
```

See {numref}(fig:neurominer_statistical_comparison): Statistical comparisons can be conducted between the model performances using a variety of statistics using the ”Statistical Comparisons” option. The results of these comparisons are output in either excel spreadsheet files (i.e., xlsx) or in text files (i.e., csv) for Linux and Mac. The menu is quite intuitive (i.e., as below) where you select the analyses you want to compare on the right, you press the ”Save As” button to choose a name, and then you select the ”Compare” button to generate the files. The files will be generated in your working directory.

```{figure} Images/NM_resultviewer_statistical_comparison.png
---
alt: Neurominer result viewer, statistical comparison
name: fig:NM_resultviewer_statistical_comparsion
---
Within the main results display, the user can go to the ”Comparisons” menu item and then select the option to calculate statistical differences between separate models
```

See {numref}`fig:NM_resultviewer_visual_comparsion`: The user can also perform ”Visual Comparisons”. The user needs to select some performance measures that they want to compare visually in the top-half of the pop-up box using the ”Select” column tick-boxes, then select the analyses in the lower-half of the pop-up box. Then simply press ”Visualize”. If the bar plots are not good then the user can change it to ”Line plots” and then press ”Visualize” again.

```{figure} Images/NM_resultviewer_visual_comparison.png
---
alt: Neurominer result viewer, visual comparisons
name: fig:NM_resultviewer_visual_comparsion
---
Within the main results display, the user can go to the ”Comparisons” menu item and then select the option to display visual comparisons between separate models
```

## Classification performance
See {numref}`fig:Display_CVperf`: NeuroMiner was built to work with hyperparameter optimization within a nested-cross validation framework as a standard. In this plot, you will see the inner-cycle, CV1, mean test performance plotted in blue and then the outer cycle, CV2, mean test performance plotted in red. Each point represents a hyperparameter combination. This gives an indication of how the hyperparameter combinations are influencing the test performance. On the right, the display also shows heatmaps that show how the performance in the CV1/CV2 folds changes over the permutations.


```{figure} Images/Display_CVperf.png
---
alt: Neurominer result viewer, cv performance plot
name: fig:Display_CVperf
---
Result viewer option 2: Cross-validation performance
```


## Generalization error
See {numref}`fig:Display_GenError`: Another way to look at CV2 test performance is with the generalization error, which is calculated in NeuroMiner as the CV1-test performance minus the CV2 test performance.

```{figure} Images/Display_GenError.png
---
alt: Neurominer result viewer, generalization error
name: fig:Display_GenError
---
Result viewer option 3: Generalization error
```

## Model complexity
See {numref}`fig:Display_Complexity`. In NeuroMiner, complexity is defined as the percentage of support vectors divided by the number of subjects in the tested fold. This feature is useful to determine how varying parameter combinations may affect the fit of the model.

```{figure} Images/Display_Complexity.png
---
alt: Neurominer result viewer, complexity
name: fig:Display_Complexity
---
Result viewer option 4: Complexity
```

## Ensemble diversity
See {numref}`fig:Display_EnsembleDiversity`: This is a measure of Shannon entropy across CV1 models for each CV2 partition. A higher entropy means higher disagreement between the CV1 models. This measure is useful because increased entropy will often result in a better fitting model at the CV2 level. It is useful to note that entropy can be maximized as a performance criterion in addition to accuracy.


```{figure} Images/Display_EnsembleDiv.png
---
alt: Neurominer result viewer, ensemble diversity
name: fig:Display_EnsembleDiv
---
Result viewer option 5: Model selection
```

## Model selection frequency
See {numref}`fig:Display_ModelSelection`: Within an ensemble framework, the models that are selected within the inner, CV1, cycle are then applied to the outer test data. For example, if it is a 10x10 CV1 framework with a winner-takes-all method of model selection, then 100 models will be applied to the held-out CV2 test fold. This plot displays the percentage of times that each parameter combination was selected.

```{figure} Images/Display_ModelSelection.png
---
alt: Neurominer result viewer, model selection
name: fig:Display_ModelSelection
---
Result viewer option 6: Model selection frequency
```

## Visualization results
See {numref}`fig:Display_VisualizationResults` to {numref}`fig:Display_VisPermPVals`: As described in [visualization](3.2.06_paramtemp_visualization_options), various graphs or maps are displayed if the visualization has been run and can be viewed when selecting *Visualization results* in the drop-down menu.

```{figure} Images/NM_Display_visualizationresults.png
---
alt: Neurominer result viewer, visualization results
name: fig:Display_VisualizationResults
---
Overall mean results including standard errors with matrix data. A similar image is displayed for the grand mean results.
```

```{figure} Images/NM_Display_visualizationresults2.png
---
alt: Neurominer result viewer, visualization results
name: fig:Display_VisualizationResults2
---
CV-ratio of feature weights with matrix data.
```

```{figure} Images/NM_Display_visualizationresults3.png
---
alt: Neurominer result viewer, visualization results
name: fig:Display_VisualizationResults3
---
```

```{figure} Images/NM_Display_visualizationresults4.png
---
alt: Neurominer result viewer, visualization results
name: fig:Display_VisualizationResults4
---
Probability of feature reliability (upper left), sign-based-consistency (upper right), sign-based consistency - Z score (lower left), sign-based consistency -log10 - P value (lower right) with matrix data.
```

```{figure} Images/NM_Display_visPermPVals.png
---
alt: Neurominer result viewer, visualization results
name: fig:Display_VisPermPVals
---
Histogram of the p-values derived from permutation testing.
```

```{figure} Images/NM_Display_visImaging.png
---
alt: Neurominer result viewer, visualization results
name: fig:Display_VisImaging
---
Histogram of the p-values derived from permutation testing.
```

## ROI mapper
For neuroimaging data, NM provides a functionality for mapping the result maps described above (e.g. cross-validation ratio, sign-based consistency) to regions of interest (ROI), as defined by a desired brain atlas. This can be especially useful for visually summarizing your results for a publication and ease of interpretation.

:::{note}
This functionality is only available in MATLAB R2021b or above.
:::

The ROI mapper can be accessed from the Visualization results window in the NeuroMiner Result Viewer by clicking on the blue button **ROI mapping** (see {numref}`fig:NM_Display_visImaging`). This will open a separate window, as shown in {numref}`fig:NM_roi_mapper`. The window has two sub-menus in which you can set up the parameters for a two-step procedure: first analyzing and then plotting the ROI percentages occupied by the selected results map, thresholded accordingly.

:::{note}
The *plotting menu* and *active button* will only appear once you run the analysis part (the mapping itself).
:::

```{figure} Images/NM_roi_mapper.png
---
alt: Neurominer result viewer, visualization results roi mapper
name: fig:NM_roi mapper
---
```

## ML interpreter (MLI) results
See {numref}`fig:NM_MLI_viewer_matrix` and {numref}`fig:NM_MLI_viewer_imaging`: If the interpretation of predictions was run, the results can be viewed either by selecting ML Interpreter results from the plot drop down menu in the Results Viewer or by clicking on the blue button labeled **I** next to the classification plot (see {numref}`fig:Display_ClassPlot`). Both of these options will open the MLI Viewer.

:::{tip}
You can also select a specific subject by clicking on its point in the classification plot before clicking the blue **I** button.‚
:::

```{figure} Images/NM_MLI_viewer_matrix.png
---
alt: Neurominer result viewer, visualization results
name: fig:NM_Display_VisImaging
---
ML Interpreter result viewer for matrix data
```

```{figure} Images/NM_MLI_viewer_imaging.png
---
alt: Neurominer result viewer, visualization results mli
name: fig:NM_MLI_viewer_imaging
---
ML Interpreter result viewer for neuroimaging data
```

## Multi-group results
See {numref}(fig:Display_MultiGroup): If a multi-group analysis is conducted, the results display will change to accommodate the groups.

```{figure} Images/Display_MultiGroup.png
---
align: left
alt: Neurominer result viewer, model selection
name: fig:Display_MultiGroup
---
Mean results with imaging data. Example of the depiction of the mean feature weights with standard errors for imaging data. A similar image is displayed for all other visualization parameters (i.e., CV-ratio, probability of feature reliability, sign-based consistency).
```
